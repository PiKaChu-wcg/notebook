# 西瓜书

# 绪论

## 基本术语

| 中文     | English             | 中文       | English               |
| -------- | ------------------- | ---------- | --------------------- |
| 数据集   | data set            |            |                       |
| 示例     | instance            | 样本       | sample                |
| 属性     | attribute           | 特征       | feature               |
| 属性值   | attribute           | value      |                       |
| 属性空间 | attribute space     | 样本空间   | sample space          |
| 特征向量 | feature vector      |            |                       |
| 维数     | dimensionality      |            |                       |
| 学习     | learning            | 训练       | training              |
| 训练数据 | training data       | 训练样本   | training sample       |
| 假设     | hypothesis          |            |                       |
| 真实     | ground-truth        |            |                       |
| 学习器   | learner             |            |                       |
| 标签     | label               | 样例       | example               |
| 标记空间 | label space         |            |                       |
| 分类     | classification      | 回归       | regression            |
| 测试     | testing             | 测试样本   | testing               |
| 聚类     | clustering          | 簇         | cluster               |
| 监督学习 | supervised learning | 无监督学习 | unsupervised learning |
| 泛化     | generalization      |            |                       |
| 分布     | distribution        |            |                       |
|          |                     |            |                       |



# 模型评估与选择

error rate $$E=\alpha/m$$ 	accuracy $$1-\alpha/m$$

training error vs generalization error

overfitting vs underfitting

### 评估方法

留出法(hold-out),交叉验证法(cross validation),自助法(bootstrapping)

### 调参(parameter tuning)

验证集(validation set)用于调参

## 性能度量(performance measure)

### 均方误差(mean squared error)

$E(f;D)=\frac{1}{m}\Sigma^m_{i=1}(f(x_i)-y_i)$

### 错误率和精度

$E(f;D)=\frac{1}{m}\Sigma^m_{i=1}I(f(x_i)\neq y_i)$

$acc(f;D)=\frac{1}{m}\Sigma^m_{i=1}I(f(x_i)= y_i)=1-E(f;D)$

### 查准率(precision),查全率(recall),F1

|          | 预测结果 |       |
| -------- | -------- | ----- |
| 真实情况 | true     | false |
| positive | TP       | FP    |
| negative | TN       | FN    |

$precision=\frac{TP}{TP+FP}$

$recall=\frac{TP}{TP+FN}$

$\frac{1}{F1}=\frac{1}{2}\cdot (\frac{1}{P}+\frac{1}{R})\Rightarrow F1=\frac{2\times P\times R}{P+R}$

$\frac{1}{F_\beta}=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})$

>   宏

$macro-precision=\frac{1}{n}\sum^n_{i=1}P_i$

$macor-recall=\frac{1}{n}\sum^n_{i=1}R_i$

$marco-F1=\frac{2\times marco-P\times marco-R}{marco-P+marco-R}$

>   微

$micro-precision=\frac{\over TP}{\overline{TP}+\overline {FP}}$

$micor-recall=\frac{\over TP}{\overline{TP}+\overline {FN}}$

$mirco-F1=\frac{2\times mirco-P\times mirco-R}{mirco-P+mirco-R}$

### ROC(receiver operating characteristic) 和 AUC(area under ROC curve)

真正比率 $TPR=\frac{TP}{TP+FN}$ true positive rate

假正比率 $FPR=\frac{FP}{TN+FP}$ false positive rate

$AUC=\frac{1}{2}\sum^{m-1}_{i=1}(x_{i+1}-x_i)\cdot (y_i+y_{i+1})$

$l_{rank}=\frac{1}{m^+m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}(I(f(x^+)<f(x^-))+\frac{1}{2}I(f(x^+)=f(x^-)))$

$AUC=1-l_{rank}$

### 代价敏感(cost-sensitive)错误率和代价曲线

$E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}I(f(x_i)\neq y_i)\times cost_{01}+\sum_{x_i\in D^-}I(f(x_i)\neq y_i)\times cost_{10})$

$cost$ 为代价矩阵

 正例代价 $P(+)cost=\frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}$

$cost_{norm}\frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}$

## 比较检验

假设检验(hypothesis test)

默认以错误率为性能度量,$\varepsilon$

### 假设检验

测试错误率$\hat\varepsilon$ 



$P(\hat\varepsilon;\varepsilon)=\begin{pmatrix}m\\\hat\varepsilon\end{pmatrix} \varepsilon^{\hat\varepsilon\times m}(1-\varepsilon)^{m-\hat\varepsilon \times m} $

二项式分布

==各种检验以后慢慢看吧==

## 方差和偏差

$\overline f(x)=E_D(f(x;D))$

$var(x)=E_D((f(x;D)-\overline f(x))^2)$

噪声

$\varepsilon^2=E_D((y_D-y)^2)$

偏差

$bias^2(x)=(\overline f(x)-y)^2$

$\therefore E(f;D)=bias^2(x)+var(x)+\varepsilon^2$

# 线性(linear)模型

## 线性回归(linear regression)

$f(x)=w^Tx+b$

$D=\{(x_i,y_i) \}^m_{i=1}$

$(w^*,b^*)=\underset{(w,b)}{argmin}\sum^m_{i=1}(f(x_i)-y_i)^2$

## 对数几率回归(logit regression)

logistic function

$y=\frac{1}{1+e^{-z}}\\z=w^Tx+b$

$\Rightarrow ln\frac{y}{1-y}=\hat w\cdot \hat x$

$y$为正例的可能$1-y$为反例的可能$\frac{y}{1-y}$称为几率 (odds)

$ln\frac{p_1(x)}{p_0(x)}=\hat w\cdot\hat x$

极大似然估计$p_1(x)=\hat w \hat x,p_0(x)=\hat w \hat x $

损失函数:$l(\hat w)=\sum^m_{i=1}\ln p(y_i|x_i;\hat w)$

其中似然项可以重写成$p(y_i|x_i;\hat w)=y_ip_1(\hat x_i;\hat w)+(1-y_i)p_0(\hat x_i;\hat w)$

### 线性判别分析(linear discriminant analysis)

$D=\{(x_i,y_i) \}^m_{i=1}$

$X_i,\mu_i,\Sigma_i$分别表示第$i\in \{0,1\}$类示例的集合,均值向量,协方差矩阵

投影到$w$上之后

$w^T\mu$   $w^T\Sigma w$

目标函数$J=\frac{||w^T\mu_0-w^T\mu_1 ||^2_2}{w^T\Sigma_0 w+w^T\Sigma_1 w}\\=\frac{||w^T\mu_0-w^T\mu_1 ||^2_2}{w^T(\Sigma_0+\Sigma_1 )w}$

类内散度矩阵(within-class scatter matrix)

$S_w=\Sigma_0+\Sigma_1\\=\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_0)(x-\mu_0)^T$

类间散度矩阵(between-class scatter matrix)

$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$

$\therefore J=\frac{w^TS_bw}{w^TS_ww}$

# 决策树

## 划分选择

结点纯度(purity)

### 信息增益(information gain)

信息熵(information entropy)

$Ent(D)=-\sum^{|y|}_{k=1}p_k\log_2p_k$

信息增益

$Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$

### 增益率(gain ratio)

$Gain_ratio=\frac{Gain(D,a)}{IV(a)}$

其中$IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}$

### 基尼系数(Gini index)

$Gini(D)=\sum^{|y|}_{k=1}\sum_{k'\neq k}p_kp_k'=1-\sum^{|y|}_{k=1}p^2_k$

$Gini_index=\sum^V_{v=1}\frac{|D^v|}{|D|}G(D^v)$

## 剪枝(pruning)处理

预剪枝 (prepruning) 后剪枝(postpruning)

泛化能力能不能继续提升

## 连续和缺失值

### 连续值

二分法(bi-partition)

$T_a={\frac{a^i+a^{i+1}}{2}|1\le i\le n-1}$

$Gain(D,a)=\underset{t \in T_a}{\max}Gain(D,a,t)$

### 缺失值处理

$Gain(D,a)=\rho\times Gain(\tilde D,a)$

$\tilde D$为$D$在属性$a$上没有缺失的样本子集

## 多变量决策树

# 神经网络(neural network)==以后看深度学习==

## 神经元(neuron)模型

激活函数(activation function)

## 感知机(perceptron)和多层网络

# 支持向量机(support vector machine)

==最近刚看过,下次复习的时候再做笔记==

# 贝叶斯(Bayesian)分类器

$x$上的条件风险$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$

贝叶斯风险$R(h)=E_x[R(h(x)|x)]$

贝叶斯最优分类器$h^*(x)=\underset{c\in y}{\arg\min}R(c|X)$

取$R(c|x)=1-P(c|x)$

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$

## 极大似然估计(maximum likelihood estimation)

$P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)$

## 朴素(naive)贝叶斯分类器

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod^d_{i=1}P(x_i|c)$

$h_{nb}(x)=\underset{c\in y}{\arg\min}P(c)\prod^d_{i=1}P(x_i|c)$

其中$P(c)=\frac{|D_c|}{|D|}$,$P(x_i|c)=\frac{|D_{c,x_i}|}{D_c}$

### ==半朴素(semi-naive)==

## ==贝叶斯网==

## EM(expectation-maximization)算法

隐变量(latent variable)

$X,Z,\Theta$分别为已观测变量集,隐变量集,模型参数

$LL(\Theta|X,Z)=\ln P(X,Z|\Theta)$

$E:\Theta^t\rightarrow Z^t\\M:X,Z^t\rightarrow\Theta^{t+1}$

# 集成学习(ensemble learning)

## boosting

$H(x)=\sum^T_{t=1}\alpha_th_t(x)$

## 多样性

### 误差-分歧(ambiguity)分解

$A(h_i|x)=(h_i(x)-H(x))^2$

$E(h_i|x)=(f(x)-h_i(x))^2$

$E(H|x)=(f(x)-H(x))^2$

# 聚类(clustering)

簇cluster

## 性能度量

有效指标(validity index)

簇内相似度(intra-cluster similarity) 簇间相似度(inter-cluster similarity)

###  外部指标(external index)

$a=|SS|,SS=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\}$

$b=|SD|,SD=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*\neq\lambda_j^*,i<j\}$

$a=|DS|,DS=\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda_i^*=\lambda_j^*,i<j\}$

jaccard coefficient 简称 $JC=\frac{a}{a+b+c}$

Fowlkes and Mallows Index, 简称$FMI=\sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}$

Rand Index $RI=\frac{2(a+b)}{m(m-1)}$



### 内部指标

$avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1\le<j\le|C|}dist(x_i,x_j)$

$diam(C)=\max_{1\le<j\le|C|}dist(x_i,x_j)$

$d_{min}(C_i,C_j)=\min_{x_i\in C_i,x_j\in C_j}dist(x_i,x_j)$

$d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)$

Davies-Bouldin index $DBI=\frac{1}{k}\sum^k_1\underset{j\neq i}{\max}(\frac{avg(C_i+avg(C_j))}{d_{cen(C_i,C_j)}})$

Dunn Index $DI=\underset{1\le i\le k}{\min}\{\underset{j\neq i}{\min}(\frac{d_{min}(C_i,C_j)}{max_{1\le l\le k}diam(C_l)})\}$

## 距离计算

距离度量(distance measure)

连续属性(continuous attribute) 离散属性(categorical attribute)

有序属性(ordinal attribute)无序属性(non-ordinal attribute)

VDM(Value Difference Metric)

$VDM_p(a,b)=\sum^k_{i=1}|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}|^p$

第$i$样本簇属性$u$上值为$a$

$MinkovDM_p(x_i,x_j)=(\sum^{n_c}_{u=1}|x_{iu}-x_{ju}|^p+\sum^n_{u=n_c+1}VDM_p(x_{iu},x_{ju}))$

## 原型聚类

### k均值(k-means)

$E=\sum^k_{i=1}\sum_{x\in C_i}||x-\mu_i||^2_2$

### 学习向量量化(Learning vector Quantization)

### 高斯混合(Mixture-of–Gaussian)聚类

高斯分布

$p(x)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}e^{-1/2(x-\mu)^T\Sigma^{-1}(x-\mu)}$

$\Sigma$是$n\times n$的协方差矩阵



$pm(x)=\sum^k_{i=1}\alpha_i\cdot p(x|\mu_i,\Sigma_i)$

==还有些没看懂==

## 密度聚类(density-based clustering)

### DBSCAN

*   $\epsilon$-邻域(neighborhood: $N_\epsilon(x_j)=\{x_i\in D|dist(x_i,x_j)\le \epsilon\}$

*   核心对象(core object):$|N_\epsilon(x_j)|\ge MinPts$,则$x_j$为一个核心对象

*   密度直达(directly density-reachable),$x_j$为核心对象,$x_i$在$x_j$的邻域中,这$x_i$由$x_j$密度直达

*   密度可达(density-reachable)

*   密度相连(density-connected)


## 层次聚类

# 降维和度量学习

## k近邻学习(k-nearest neighbor)

$P(err)=1-\sum_{c\in Y}P(c|x)P(c|z)\approx 1-\sum_{c\in Y}P^2(c|x)\le 1-P^2(c^*|x) \le 2\times (1-P(c^*|x))$

## 低维嵌入

$dist_{ij} =||z_i||^2+||z_j||^2-2z_i^Tz_j\\ =b_{ii}+b_{jj}-2b_{ij}$

$tr(B)=\sum^m_{i=1}||z_i||^2$

$\sum^m_{i=1}dist^2_{ij}=tr(B)+mb_{jj}$

$\sum^m_{i=1}\sum^m_{j=1}dist^2_{ij}=2m\:tr(B)$

$b_{ij}=-\frac{1}{2}(dist^2_{ij}-dist^2_{.j}-dist^2_{i.}+dist^2_{..})$

对$B$矩阵做特征值分解(eigenvalue decomposition)

$B=V\Lambda V^T$

$Z=\Lambda_*^{1/2}V_*^T$

## 主成分分析(Principal Component Analysis)

*   最近重构性

*   最大可分性 

$$
\sum^m_{i=1}||\sum^{d'}_{j=1}z_{ij}w_j-x_i||^2_2=\sum^m_{i=1}z_i^TW^Tx_i+const $\propto$z_i^Tz_i-2\sum^m_{i=1}z_i^TW^Tx_i+const\\ \propto -tr(W^T(\sum^m_{i=1}x_ix_i^T)W)
$$

$$
\underset{W}{\max}\: tr(W^TXX^TW)\\ s.t.\: W^TW=I
$$



$XX^TW=\lambda W$

## 核化线性降维(Kernelized PCA)

$(\sum^m_{i=1}\phi(x_i)\phi(x_i)^T)W=\lambda W$

## 流形学习

## 度量学习(metric learning)

$dist_{ed}^2(x_i,x_j)=||x_i-x_j||^2_2=dist^2_{ij,1}+dist^2_{ij,2}+...+dist^2_{ij,d}$

$dist_{wed}^2(x_i,x_j)=||x_i-x_j||^2_2=w_1\cdot dist^2_{ij,1}+w_2\cdot dist^2_{ij,2}+...+w_d\cdot dist^2_{ij,d}\\=(x_i-x_j)^TW(x_i-x_j)=||x_i-x_j||^2_W$

$W=diag(w)$是一个对角矩阵

$dist^2_{mah}(x_i,x_j)=(x_i-x_j)^TM(x_i-x_j)$

$M$是一个半正定矩阵

# 特征选择(feature selection)与稀疏学习(sparse learning)

## 子集搜索(subset search)与评价(evaluation)

$Gain(A)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$

## 过滤式,包裹式,嵌入式

## 系数表示与字典学习

==再看==



# *计算学习理论(computational learning theory)* 

泛化误差为:$E(h;D)=P_{x\sim D}(h(x)\neq y)$

经验误差为:$\hat E(h;D)=\frac{1}{m}\sum^m_{i=1}I(h(x_i)\neq y_i)$

Hoeffding不等式:$P(\frac{1}{m}\sum^m_{i=1}x_i-\frac{1}{m}\sum^m_{i=1}E(x_i)\ge \epsilon)\le \exp(-2m\epsilon)$

## PAC(Probably Approximately Correct)学习

==先放着吧==

# *半监督学习(semi-supervised learning)*

聚类假设(cluster assumption)

流形假设(manifold assumption)

## 生成式方法(generative method)

$p(x)=\sum^N_{i=1}\alpha_ip(x|\mu_i,\Sigma_i)$

$p(x|\mu_i,\Sigma_i)$为样本$x$属于第$i$个高斯混合成分的概率	

$f(x)=\underset{j\in y}{\arg\min}p(y=j|x)\\=\underset{j\in y}{\arg\min}\sum^N_{i=1}p(y=j,\Theta=i|x)\\=\underset{j\in y}{\arg\min}\sum^N_{i=1}p(y=j|\Theta=i,x)\cdot p(\Theta=i|x)$

其中$p(\Theta=i|x)=\frac{\alpha_ip(x|\mu_i,\Sigma_i)}{\sum^N_{i=1}\alpha_ip(x|\mu_i,\Sigma_i)}$

$LL(D_l,D_u)=\sum_{(x_i,y_j)\in D_l}\ln (\sum^N_{i=1}p(y=j,\Theta=i|x))\\+\sum_{x_j\in D_u}\ln \sum^N_{i=1}\alpha_ip(x|\mu_i,\Sigma_i)$

## 半监督SVM(semi-supervised support vector machine)

$\underset{w,b,\hat y,\xi}{\min}\frac{1}{2}||w||^2_2+C_l\sum^l_{i=1}\xi_i+C_u\sum^m_{i=l+1}\xi_i$

$s.t.\: y_i(w^Tx_i+b)\ge1-\xi_i$

$\hat y_i(w^Tx_i+b)\ge1-\xi_i$

$\xi_i\ge 0$

## 图半监督学习

## 基于分歧的方法

## 半监督聚类

约束k均值聚类

每次更新时要检查是否违反约束条件

# 概率图模型

## 隐马尔科夫模型(hidden markov model)

$P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1|y_1)\prod^n_{i=2}P(y|i|y_{i-1})P(x_i|y_i)$

$a_{ij}=P(y_{t+1}=s_j|y_t=s_i)$

$b_{ij}=P(x_t=o_j|y_t=s_i)$

$\pi_i=P(y_i=s_i)$

$\lambda=[A,B,\pi]$

## 马尔科夫随机场(markov random field)

$P(x)=\frac{1}{Z}\prod_{Q\in C}\psi_Q(x_Q)$





# *规则学习(rule learning)*



# 强化学习(reinforcement learning)

## 任务与奖励(reward)

马尔科夫决策过程(markov decision process)

$E=(X,A,P,R)$

$X$环境描述$A$动作空间 $P$转移概率,$R$奖励

## K-摇臂赌博机

### $\epsilon$-贪心

每次以$\epsilon$的概率进行探索,$1-\epsilon$的概率进行利用

### softmax

$P(k)=\frac{e^{Q(k)/\tau}}{\sum^K_{i=1}e^{Q(i)/\tau}} $

## 有模型学习

$V^\pi_T(x,a)=E_\pi[\frac{1}{T}\sum^T_{t=1}r_t|x_0=x]=\sum_{a\in A}\pi(x,a)\sum_{x'\in X}P^a_{x'\rightarrow x}(\frac{1}{T}R^a_{x'\rightarrow x}+\frac{T-1}{T}V^\pi_{T-1}(x')$

$V^\pi_\gamma(x,a)=E_\pi[\sum^{+\infin}_{t=0}\gamma^t r_t+1|x_0=x]=\sum_{a\in A}\pi(x,a)\sum_{x'\in X}P^a_{x'\rightarrow x}(R^a_{x'\rightarrow x}+\gamma V^\pi_{T-1}(x')$

$Q^\pi_T(x,a)=E_\pi[\frac{1}{T}\sum^T_{t=1}r_t|x_0=x,a_0=a]$

$Q^\pi_\gamma(x,a)=E_\pi[\sum^{+\infin}_{t=0}\gamma^t r_t+1|x_0=x,a_0=a]$

策略改进$\pi^*=\underset{\pi}{argmax}\sum_{x\in X}V^\pi(x)$

## 免模型学习(model-free learning)

### 蒙特卡洛强化学习

### 时序差分(temporal difference)学习

### 值函数近似

## 模仿学习(imitation learning)





