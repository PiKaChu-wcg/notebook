# 卷积神经网络

## 卷积基础知识

### conv和FC的区别

* 局部连接
* 共享权值
* 输入/输出结构化

### 感受野大小

$R^{i}_e=min(R^{i-1}_e+(k^i_e-1)\prod^{i-1}_{j=0}s^j_e,L_e)$

$k$ 尺寸, $s$ 步长

### 卷积输出尺寸,参数量和计算量

输出尺寸$l^o_e=[\frac{l^i_e+2p_e-k_e}{s_e}]+1$

参数量$c^ic^ok_wk_h$

计算量$c^ic^ol^i_wl^i_hk_wk_h/(s_ws_h)$

## 卷积的变种

### 分组卷积

alexnet 分组通道进行卷积实现参数量和计算量都成为$1/g$,其中$g$为组数

### 转置卷积

反卷积,中间填充$s-1$个零,边缘填充$k-p-1$个零

### 空洞卷积

提升感受野

## 神经网络的整体结构

### alexnet到resnet

#### alexnet

* 修正线性单元ReLU
* 局部响应归一化LRN
* Dropout
* 数据扩充
* 分组卷积

#### VGGnet

* $3\times 3$卷积
* $2\times 2$池化
* 去掉了LRN

#### Googlenet/inception

* bottleneck在计算大卷积前会用$1\times 1$卷积对通道进行压缩,最后多个通道拼接
* 中间特征连接辅助分类器
* 全局平均池化

#### inceptionv2/inceptionv3

* 避免表示瓶颈,即信息表征的突然降低

* 特征通达多

* 空间域上的聚合操作前把通道进行压缩不会影响信息表征能力

* 深度和宽度的平衡

  

#### Resnet

* 拟合残差

#### inceptionv4-inceptionResnet



## 卷积神经网络的基础模块

### 批归一化

* 网络每一层都要不断的适应输入数据的不同的分布
* 前几层使得后面的参数掉入激活函数的饱和区
* 为了避免上述问题,学习率小

$y^k=\gamma^k\frac{x^k-\mu^k}{\sqrt{\sigma^k+\epsilon}}+\beta^k$

* 得到不同的分布,保留网络学习的成果
* 让参数落入激活函数的非线性部分
* eval()

### 全局平均池化层

在卷积和FC之间,减少计算量,增加可解释性(知道哪个特征图贡献大)

### 瓶颈结构和沙漏结构

#### 瓶颈结构

大卷积前用$1\times 1$卷积减少通道,然后再用$1\times 1$卷积再复原

#### 沙漏

encoder-decoder

# 循环神经网络

## 循环神经网络和序列建模

### 参数更新方法

先计算$\frac{\partial L}{\partial h_n}$再算$\frac{\partial L}{\partial h_{n-1}}$

### 卷积神经网络对序列建模

TextCNN,二维卷积

## 循环神经网络中的dropout

### dropout如何缓解过拟合

网络中的神经元不会对一个特定的神经元特别敏感,使得网络学习到一些更加泛化的特征

### 循环神经网络中使用dropout

这东西不理想,滚!

## 循环神经网络中的长期依赖

梯度爆炸的问题在长期依赖中依旧存在

## 长短期记忆网络

### LSTM

$i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)$ 输入

$f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)$ 遗忘

$o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)$ 输出

$\tilde{c}=Tanh(W_cx_t+U_ch_{t-1}+b_c)$ 控制

$c=f_t\odot c_{t-1}+i_t\odot \tilde{c_t} $

### GRU

$r_t=\sigma(W_rx_t+U_rh_{t-1})$  重置

$z_t=\sigma(W_zx_t+U_zh_{t-1})$ 更新

$\tilde{h_t}=Tanh(W_hx_t+U_h(r_t\odot h_{t-1}))$

$h_t=(1-z_t)h_{t-1}+z_t\tilde{h}_t$

## seq2seq架构

废话!

# 图神经网络

## 图神经网络的基本结构

### 图谱和图傅里叶变换

拉普拉斯矩阵$L=D-A$其中$D=diag(d_1,d_2,...,d_n)$为度矩阵.$A$为连接矩阵

对其进行特征值分解$L=U\Lambda U^T$

$\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_n)$是从大到小排列的

$U=[u_1,u_2,...,u_n]$是其特征矩阵

$\{\lambda_1,\lambda_2,...,\lambda_n\}$就是图谱



最后傅里叶变换:$\hat x= U^Tx$逆变换:$x=U\hat x$

### GCN

$\hat x=U^T x,\hat h=U^Th$

$h*x=U\cdot diag(\hat h)\cdot U^Tx $

用$\sum_{k=0}^K\alpha_k\Lambda^k$代替$diag(\hat h)$

最后$y=\sigma(\sum_{k=0}^K\alpha_kL^kx)$

### GAT和GraphSAGE

#### GAT

$y_i=\sigma(\frac{1}{M}\sum_{m=1}^M\sum_{j\in N_i}\alpha_{ij}^{(m)}(x_j\cdot W^{(m)}))$

#### GrapgSAGE

$h_{N_i}=aggregate(\{x_j|j\in N_i\})$

$y_i=\sigma(concat(h_i,h_{N_i})\cdot W)$

## 图神经网络在推荐系统上的应用

### pinsage

二部图:用户和图片(文本等等)

嵌入

* 随机游走
* 加权平均实现aggregate
* 负样本的选取,最大间隔函数(max-margin loss),选取hard negative

## 图神经网络的推理能力

### 图神经网络在推理框架的优势

* 推断关系的能力
* 充分利用数据
* 输出和节点顺序无关

### 图神经网络的推理机制在其他领域的应用

* 注意力机制
* 基于度量的元学习
* 分解机

# 生成模型

## 深度信念网络与深度玻尔兹曼机

看不懂

### RBM(Restricted Boltzmann Manchine)

$p(v,h)=\frac{1}{Z}exp(-E(v,h))$

$E(v,h)=\sum_{i,j}w_{ij}v_ih_j-\sum_ib_iv_i-\sum_jc_jh_j$

以极大似然作为目标

$\frac{\partial log(p(v))}{w_{ij}}=<v_i,h_i>_{data}-<v_i,h_i>_{model}$

### DBN和DBM

看不懂

## 变分自编码器

### VAE

先验$p_\theta(z)$条件分布$p_\theta(x|z)$

后验$Q_\phi(z|x)$

$log(P_\theta(x_i))\ge log(P_\theta(x_i))-KL(Q_\phi(z|x)||p_\theta(x|z))\\=E_{z-Q}[log(P_\theta(x_i|z))]-KL(Q_\phi(z|x_i)||P_\theta(z))$

### VAE控制生成图像的类别

增加隐变量$y$

### 改进VAE,编码解耦

增加KL散度前的系数

## 变分自动编码器的改进

### 原始VAE的问题

* 真实的后验分布不一定满足高斯分布
* 对数似然$log(P_\theta(x))$和下界有距离,然而VAE是在优化他的下界

## VAE和GAN结合

$E_{z-Q}[log(P_\theta(x_i|z))]-KL(Q_\phi(z|x_i)||P_\theta(z))$

第一项是重构损失,第二项是KL散度,起正则化作用

KL散度希望分布隐变量分布接近先验分布,这步可以用GAN来替代

## 生成式矩匹配网络和深度自回归网络

### 最大均值差异

$L_{MMD^2}=||\frac{1}{N}\sum_{i=1}^N\phi(x_i)-\frac{1}{M}\sum_{j=1}^M\phi(y_j)||^2$

$=\frac{1}{N^2}\sum_{i,i'}k(x_i,x_i')-\frac{1}{NM}\sum_{i,j}k(x_i,y_j)+\frac{1}{M^2}\sum_{j,j'}k(y_j,y_j')$

通常$k(x,y)=exp(-\frac{||x-y||^2}{2\sigma^2})$高斯核

### 自回归方法应用在生成模型

贝叶斯网络

# 强化学习

## 强化学习基础

### 马尔科夫

智能体agent

环境environment

* 状态集合S
* 动作集合A
* 状态转移函数P
* 奖励函数R

动态规划

蒙特卡洛

### 有模型学习和免模型学习

免模型:和真实环境交互

有模型,和建立的模型交互

### 基于策略迭代和基于价值迭代

策略:根据策略的规则选取动作

价值:面向动作的

## 强化学习算法

### 时序差分和蒙特卡洛

在一次过程的中途

时序差分会根据之前的情况来改变策略

蒙特卡洛只能在一次完整的过程之后改变策略

### Q-learning

$Q_\pi(s_t,a_t)=E_\pi[G_t|s_t,a_t]=E_\pi[r_t+\gamma Q_\pi(s_{t+1},a_{t+1})|s_t,a_t]$

$Q_\pi(s_t,a_t)\leftarrow Q_\pi(s_t,a_t)+a(r_t+\gamma max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1})-Q_\pi(s_t,a_t))$

### Sarsa和Sarsa($\lambda$)

相对Q-learing每步都贪心的取$max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1})$

Sarsa相对保守选择安全的迭代路径,沿用一直的策略,只是进化策略的参数

## 强化深度学习

### DQN(deel Q-network)

利用深度学习拟合策略函数

## 强化学习的应用

* 游戏
* 自动驾驶
* 神经网络架构搜索
* 自然语言处理的对话系统
* 广告主竞价策略

# 生成式对抗神经网络

## 生成式对抗神经网络基本原来

### AE,VAE,GAN

#### AE

$x\rightarrow z \rightarrow \hat x$

去噪,可视化降维,数据生成

#### VAE

使得$p(z|x)$接近一个特定的分布,使得整个空间中的向量都是有效的

#### GAN

$\underset{G}{min}\:\underset{D}{max}V(G,D)=E_{x-p_{data}}[log(D(x))]+E_{z-p_z(z)}[log(D(G(z)))]$

### GAN的理论问题

$D^*_G(x)=\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$

$L(G)=\underset{D}{max}V(G,D_G^*)\\=E_{x-p_{data}}[log(D(x))]+E_{z-p_z(z)}[log(1-D(x)]\\=E_{x-p_{data}}[log\frac{p_{data}(x)}{(p_{data}(x)+p_g(x))/2}]+E_{x-p_{g}}[log\frac{p_{g}(x)}{(p_{data}(x)+p_g(x))/2}]-log4\\=2\cdot JS(p_{data}||p_g)-2log2$

其中$JS(p_1||p_2)=\frac{1}{2}KL(p_1||\frac{p_1+p_2}{2})+\frac{1}{2}KL(p_2||\frac{p_1+p_2}{2})$

在$p_{data}和p_g$基本没有重叠的时候,导数基本为0



因此

$L(G)=E_{x-p_{data}}[log(D(x))]+E_{z-p_z(z)}[log(D(G(z)))]$

依旧有问题



### 原始的GAN在应用中的问题

$\underset{G}{min}\:\underset{D}{max}V(G,D)\neq\underset{D}{max}\:\underset{G}{min}V(G,D)$

* 不能保证实际应用的收敛
* 博弈过程陷入纳什均衡,出现模式坍塌
* 没有理想的评价标准

## 生成式对抗神经网络的改进

### GAN目标函数的演化

#### 基于f-散度的GAN

$D_f(p_{data}|p_g)=E_{x\sim p_g(x)}[f(\frac{p_{data}}{p_g})]$

#### 基于积分概率度量的GAN

$D_F(p_{data},p_g)=\underset{f\sim F}{sup}\{E_{x\sim p_{data}}[f(x)]-E_{x\sim p_{g}}[f(x)]\}$

### GAN模型结构的演变

DCGAN

AE

Stacked GAN:多个

progressive GAN:逐渐

### 最近几年对GAN的trick

特征匹配技术

单边标签平滑

谱归一化

## 生成式对抗网络的效果评估

### IS

$exp(E_{x\sim p_{g}}KL(p(y|x)||p(y)))$

$p(y)$是用分类器分类的结果

### FID

不用最后的结果,用前一层的特征













