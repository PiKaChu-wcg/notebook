# 卷积神经网络

## 卷积基础知识

### conv和FC的区别

* 局部连接
* 共享权值
* 输入/输出结构化

### 感受野大小

$R^{i}_e=min(R^{i-1}_e+(k^i_e-1)\prod^{i-1}_{j=0}s^j_e,L_e)$

$k$ 尺寸, $s$ 步长

### 卷积输出尺寸,参数量和计算量

输出尺寸$l^o_e=[\frac{l^i_e+2p_e-k_e}{s_e}]+1$

参数量$c^ic^ok_wk_h$

计算量$c^ic^ol^i_wl^i_hk_wk_h/(s_ws_h)$

## 卷积的变种

### 分组卷积

alexnet 分组通道进行卷积实现参数量和计算量都成为$1/g$,其中$g$为组数

### 转置卷积

反卷积,中间填充$s-1$个零,边缘填充$k-p-1$个零

### 空洞卷积

提升感受野

## 神经网络的整体结构

### alexnet到resnet

#### alexnet

* 修正线性单元ReLU
* 局部响应归一化LRN
* Dropout
* 数据扩充
* 分组卷积

#### VGGnet

* $3\times 3$卷积
* $2\times 2$池化
* 去掉了LRN

#### Googlenet/inception

* bottleneck在计算大卷积前会用$1\times 1$卷积对通道进行压缩,最后多个通道拼接
* 中间特征连接辅助分类器
* 全局平均池化

#### inceptionv2/inceptionv3

* 避免表示瓶颈,即信息表征的突然降低

* 特征通达多

* 空间域上的聚合操作前把通道进行压缩不会影响信息表征能力

* 深度和宽度的平衡

  

#### Resnet

* 拟合残差

#### inceptionv4-inceptionResnet



## 卷积神经网络的基础模块

### 批归一化

* 网络每一层都要不断的适应输入数据的不同的分布
* 前几层使得后面的参数掉入激活函数的饱和区
* 为了避免上述问题,学习率小

$y^k=\gamma^k\frac{x^k-\mu^k}{\sqrt{\sigma^k+\epsilon}}+\beta^k$

* 得到不同的分布,保留网络学习的成果
* 让参数落入激活函数的非线性部分
* eval()

### 全局平均池化层

在卷积和FC之间,减少计算量,增加可解释性(知道哪个特征图贡献大)

### 瓶颈结构和沙漏结构

#### 瓶颈结构

大卷积前用$1\times 1$卷积减少通道,然后再用$1\times 1$卷积再复原

#### 沙漏

encoder-decoder

# 循环神经网络

## 循环神经网络和序列建模

### 参数更新方法

先计算$\frac{\partial L}{\partial h_n}$再算$\frac{\partial L}{\partial h_{n-1}}$

### 卷积神经网络对序列建模

TextCNN,二维卷积

## 循环神经网络中的dropout

### dropout如何缓解过拟合

网络中的神经元不会对一个特定的神经元特别敏感,使得网络学习到一些更加泛化的特征

### 循环神经网络中使用dropout

这东西不理想,滚!

## 循环神经网络中的长期依赖

梯度爆炸的问题在长期依赖中依旧存在

## 长短期记忆网络

### LSTM

$i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)$ 输入

$f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)$ 遗忘

$o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)$ 输出

$\tilde{c}=Tanh(W_cx_t+U_ch_{t-1}+b_c)$ 控制

$c=f_t\odot c_{t-1}+i_t\odot \tilde{c_t} $

### GRU

$r_t=\sigma(W_rx_t+U_rh_{t-1})$  重置

$z_t=\sigma(W_zx_t+U_zh_{t-1})$ 更新

$\tilde{h_t}=Tanh(W_hx_t+U_h(r_t\odot h_{t-1}))$

$h_t=(1-z_t)h_{t-1}+z_t\tilde{h}_t$

## seq2seq架构

废话!



